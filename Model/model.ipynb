{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML Libraries\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "# Time Series Libraries\n",
        "try:\n",
        "    from statsmodels.tsa.arima.model import ARIMA\n",
        "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "except ImportError:\n",
        "    print(\"Warning: statsmodels not available - ARIMA/SARIMA models will be skipped\")\n",
        "\n",
        "# Deep Learning Libraries\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
        "    from tensorflow.keras.callbacks import EarlyStopping\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    tf.random.set_seed(42)\n",
        "except ImportError:\n",
        "    print(\"Warning: TensorFlow not available - LSTM/GRU models will be skipped\")\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import os"
      ],
      "metadata": {
        "id": "sDZvlvCssPO6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wJxwfowGsKhG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class ForecastingPipeline:\n",
        "    def __init__(self, data_path):\n",
        "        self.data_path = data_path\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "        self.target_columns = ['usage_cpu', 'usage_storage', 'users_active']\n",
        "        self.feature_columns = []\n",
        "\n",
        "        # Create directories for saving models\n",
        "        os.makedirs('models', exist_ok=True)\n",
        "        os.makedirs('results', exist_ok=True)\n",
        "        os.makedirs('top_models', exist_ok=True)\n",
        "\n",
        "        self.hyperparameter_results = {}  # Store hyperparameter search results\n",
        "\n",
        "    def load_and_prepare_data(self):\n",
        "        \"\"\"Load and prepare data for training\"\"\"\n",
        "        print(\"Loading and preparing data...\")\n",
        "        self.df = pd.read_csv(self.data_path)\n",
        "        self.df['date'] = pd.to_datetime(self.df['date'])\n",
        "        self.df = self.df.sort_values(['unique_id', 'date'])\n",
        "\n",
        "        # Define feature columns (exclude targets and non-feature columns)\n",
        "        exclude_cols = ['date', 'unique_id'] + self.target_columns\n",
        "        # Only select numeric columns for features\n",
        "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
        "        self.feature_columns = [col for col in numeric_cols if col not in exclude_cols]\n",
        "\n",
        "        print(f\"Data loaded: {self.df.shape}\")\n",
        "        print(f\"Feature columns: {len(self.feature_columns)}\")\n",
        "        print(f\"Target columns: {self.target_columns}\")\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def create_train_val_test_split(self, df, train_ratio=0.7, val_ratio=0.2):\n",
        "        \"\"\"Create train/validation/test splits by unique_id\"\"\"\n",
        "        train_data, val_data, test_data = [], [], []\n",
        "\n",
        "        for unique_id in df['unique_id'].unique():\n",
        "            group_data = df[df['unique_id'] == unique_id].sort_values('date')\n",
        "            n = len(group_data)\n",
        "\n",
        "            train_end = int(n * train_ratio)\n",
        "            val_end = int(n * (train_ratio + val_ratio))\n",
        "\n",
        "            train_data.append(group_data.iloc[:train_end])\n",
        "            val_data.append(group_data.iloc[train_end:val_end])\n",
        "            test_data.append(group_data.iloc[val_end:])\n",
        "\n",
        "        train_df = pd.concat(train_data, ignore_index=True)\n",
        "        val_df = pd.concat(val_data, ignore_index=True)\n",
        "        test_df = pd.concat(test_data, ignore_index=True)\n",
        "\n",
        "        print(f\"Train: {len(train_df)} samples\")\n",
        "        print(f\"Validation: {len(val_df)} samples\")\n",
        "        print(f\"Test: {len(test_df)} samples\")\n",
        "\n",
        "        return train_df, val_df, test_df\n",
        "\n",
        "    def calculate_metrics(self, y_true, y_pred, model_name, target):\n",
        "        \"\"\"Calculate evaluation metrics\"\"\"\n",
        "        # Handle any NaN or infinite values\n",
        "        mask = ~(np.isnan(y_true) | np.isnan(y_pred) | np.isinf(y_true) | np.isinf(y_pred))\n",
        "        y_true_clean = y_true[mask]\n",
        "        y_pred_clean = y_pred[mask]\n",
        "\n",
        "        if len(y_true_clean) == 0:\n",
        "            return {'MAE': np.nan, 'RMSE': np.nan, 'MAPE': np.nan, 'Bias': np.nan}\n",
        "\n",
        "        mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
        "        rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
        "\n",
        "        # MAPE - handle division by zero\n",
        "        mape = np.mean(np.abs((y_true_clean - y_pred_clean) / np.where(y_true_clean != 0, y_true_clean, 1))) * 100\n",
        "\n",
        "        # Forecast Bias\n",
        "        bias = np.mean(y_pred_clean - y_true_clean)\n",
        "\n",
        "        return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape, 'Bias': bias}\n",
        "\n",
        "    def aggregate_predictions_by_date(self, df, predictions, target):\n",
        "        \"\"\"Aggregate predictions by date (key improvement from standalone scripts)\"\"\"\n",
        "        temp_df = df.copy()\n",
        "        temp_df['predictions'] = predictions\n",
        "\n",
        "        # Aggregate by date: mean of actuals and predictions\n",
        "        agg_df = temp_df.groupby('date').agg({\n",
        "            target: 'mean',\n",
        "            'predictions': 'mean'\n",
        "        }).reset_index()\n",
        "\n",
        "        return agg_df[target].values, agg_df['predictions'].values\n",
        "\n",
        "    def train_xgboost_grid_search(self, train_df, val_df, test_df):\n",
        "        \"\"\"Train XGBoost with comprehensive hyperparameter grid search\"\"\"\n",
        "        print(\"\\nTraining XGBoost with Grid Search (this may take a while)...\")\n",
        "\n",
        "        # Prepare features\n",
        "        X_train = train_df[self.feature_columns].fillna(0)\n",
        "        X_val = val_df[self.feature_columns].fillna(0)\n",
        "        X_test = test_df[self.feature_columns].fillna(0)\n",
        "\n",
        "        # Define hyperparameter grid\n",
        "        param_grid = {\n",
        "            'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "            'max_depth': [3, 4, 5, 6],\n",
        "            'n_estimators': [200, 500, 800, 1000],\n",
        "            'subsample': [0.6, 0.7, 0.8, 0.9],\n",
        "            'colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n",
        "            'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
        "            'reg_lambda': [1.0, 2.0, 3.0, 5.0],\n",
        "            'gamma': [0, 0.1, 0.2, 0.5],\n",
        "            'min_child_weight': [1, 3, 5, 7]\n",
        "        }\n",
        "\n",
        "        from itertools import product\n",
        "        import random\n",
        "\n",
        "        # Generate all possible combinations\n",
        "        keys = list(param_grid.keys())\n",
        "        combinations = list(product(*[param_grid[key] for key in keys]))\n",
        "\n",
        "        # Randomly sample 50 combinations to make it manageable\n",
        "        if len(combinations) > 50:\n",
        "            combinations = random.sample(combinations, 50)\n",
        "\n",
        "        for target in self.target_columns:\n",
        "            print(f\"\\n  Grid Search for {target}...\")\n",
        "            print(f\"  Testing {len(combinations)} parameter combinations...\")\n",
        "\n",
        "            y_train = train_df[target].fillna(train_df[target].mean())\n",
        "            y_val = val_df[target].fillna(val_df[target].mean())\n",
        "            y_test = test_df[target].fillna(test_df[target].mean())\n",
        "\n",
        "            best_mae = float('inf')\n",
        "            best_params = None\n",
        "            best_model = None\n",
        "\n",
        "            for i, combo in enumerate(combinations):\n",
        "                if i % 10 == 0:\n",
        "                    print(f\"    Progress: {i}/{len(combinations)}\")\n",
        "\n",
        "                # Create parameter dictionary\n",
        "                params = dict(zip(keys, combo))\n",
        "                params.update({\n",
        "                    'objective': 'reg:squarederror',\n",
        "                    'random_state': 42,\n",
        "                    'early_stopping_rounds': 20\n",
        "                })\n",
        "\n",
        "                try:\n",
        "                    # Train model\n",
        "                    model = xgb.XGBRegressor(**params)\n",
        "                    model.fit(X_train, y_train,\n",
        "                             eval_set=[(X_val, y_val)],\n",
        "                             verbose=False)\n",
        "\n",
        "                    # Predict\n",
        "                    val_pred = model.predict(X_val)\n",
        "                    test_pred = model.predict(X_test)\n",
        "\n",
        "                    # Aggregate by date (KEY IMPROVEMENT)\n",
        "                    val_actual_agg, val_pred_agg = self.aggregate_predictions_by_date(val_df, val_pred, target)\n",
        "                    test_actual_agg, test_pred_agg = self.aggregate_predictions_by_date(test_df, test_pred, target)\n",
        "\n",
        "                    # Calculate metrics on aggregated data\n",
        "                    val_metrics = self.calculate_metrics(val_actual_agg, val_pred_agg, 'XGBoost_GridSearch', target)\n",
        "                    test_metrics = self.calculate_metrics(test_actual_agg, test_pred_agg, 'XGBoost_GridSearch', target)\n",
        "\n",
        "                    # Check if this is the best model\n",
        "                    if val_metrics['MAE'] < best_mae:\n",
        "                        best_mae = val_metrics['MAE']\n",
        "                        best_params = params.copy()\n",
        "                        best_model = model\n",
        "                        best_val_metrics = val_metrics\n",
        "                        best_test_metrics = test_metrics\n",
        "\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "            # Store best model and results\n",
        "            if best_model is not None:\n",
        "                model_key = f'XGBoost_GridSearch_{target}'\n",
        "                self.models[model_key] = best_model\n",
        "\n",
        "                # Save model\n",
        "                joblib.dump(best_model, f'models/{model_key}.pkl')\n",
        "\n",
        "                # Store in results\n",
        "                self.results[model_key] = {\n",
        "                    'validation': best_val_metrics,\n",
        "                    'test': best_test_metrics,\n",
        "                    'best_params': best_params\n",
        "                }\n",
        "\n",
        "                # Store hyperparameter results\n",
        "                self.hyperparameter_results[target] = {\n",
        "                    'best_params': best_params,\n",
        "                    'best_metrics': {\n",
        "                        'validation': best_val_metrics,\n",
        "                        'test': best_test_metrics\n",
        "                    },\n",
        "                    'total_combinations_tested': len(combinations)\n",
        "                }\n",
        "\n",
        "                print(f\"    Best MAE for {target}: {best_mae:.4f}\")\n",
        "                print(f\"    Best params: {best_params}\")\n",
        "\n",
        "        # Save best hyperparameter results\n",
        "        import json\n",
        "        with open('results/best_hyperparameter_results.json', 'w') as f:\n",
        "            json_results = {}\n",
        "            for target, data in self.hyperparameter_results.items():\n",
        "                json_results[target] = {\n",
        "                    'best_params': data['best_params'],\n",
        "                    'best_val_mae': data['best_metrics']['validation']['MAE'],\n",
        "                    'best_test_mae': data['best_metrics']['test']['MAE'],\n",
        "                    'total_combinations_tested': data['total_combinations_tested']\n",
        "                }\n",
        "            json.dump(json_results, f, indent=2)\n",
        "\n",
        "    def train_xgboost_models(self, train_df, val_df, test_df):\n",
        "        \"\"\"Train XGBoost models with default parameters (for comparison)\"\"\"\n",
        "        print(\"\\nTraining XGBoost models with default parameters...\")\n",
        "\n",
        "        # Prepare features\n",
        "        X_train = train_df[self.feature_columns].fillna(0)\n",
        "        X_val = val_df[self.feature_columns].fillna(0)\n",
        "        X_test = test_df[self.feature_columns].fillna(0)\n",
        "\n",
        "        for target in self.target_columns:\n",
        "            print(f\"  Training XGBoost for {target}...\")\n",
        "\n",
        "            y_train = train_df[target].fillna(train_df[target].mean())\n",
        "            y_val = val_df[target].fillna(val_df[target].mean())\n",
        "            y_test = test_df[target].fillna(test_df[target].mean())\n",
        "\n",
        "            # XGBoost with default parameters\n",
        "            model = xgb.XGBRegressor(\n",
        "                objective='reg:squarederror',\n",
        "                random_state=42,\n",
        "                n_estimators=200,\n",
        "                max_depth=6,\n",
        "                learning_rate=0.1,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                early_stopping_rounds=20\n",
        "            )\n",
        "\n",
        "            model.fit(X_train, y_train,\n",
        "                     eval_set=[(X_val, y_val)],\n",
        "                     verbose=False)\n",
        "\n",
        "            # Predictions\n",
        "            val_pred = model.predict(X_val)\n",
        "            test_pred = model.predict(X_test)\n",
        "\n",
        "            # Aggregate by date (KEY IMPROVEMENT)\n",
        "            val_actual_agg, val_pred_agg = self.aggregate_predictions_by_date(val_df, val_pred, target)\n",
        "            test_actual_agg, test_pred_agg = self.aggregate_predictions_by_date(test_df, test_pred, target)\n",
        "\n",
        "            # Store model and results\n",
        "            model_key = f'XGBoost_Default_{target}'\n",
        "            self.models[model_key] = model\n",
        "\n",
        "            # Save model\n",
        "            joblib.dump(model, f'models/{model_key}.pkl')\n",
        "\n",
        "            # Calculate metrics\n",
        "            val_metrics = self.calculate_metrics(val_actual_agg, val_pred_agg, 'XGBoost_Default', target)\n",
        "            test_metrics = self.calculate_metrics(test_actual_agg, test_pred_agg, 'XGBoost_Default', target)\n",
        "\n",
        "            self.results[model_key] = {\n",
        "                'validation': val_metrics,\n",
        "                'test': test_metrics\n",
        "            }\n",
        "\n",
        "    def train_lightgbm_grid_search(self, train_df, val_df, test_df):\n",
        "        \"\"\"Train LightGBM with hyperparameter grid search\"\"\"\n",
        "        print(\"\\nTraining LightGBM with Grid Search...\")\n",
        "\n",
        "        X_train = train_df[self.feature_columns].fillna(0)\n",
        "        X_val = val_df[self.feature_columns].fillna(0)\n",
        "        X_test = test_df[self.feature_columns].fillna(0)\n",
        "\n",
        "        # Define hyperparameter grid for LightGBM\n",
        "        param_grid = {\n",
        "            'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "            'max_depth': [3, 5, 7, -1],\n",
        "            'n_estimators': [200, 500, 800],\n",
        "            'num_leaves': [31, 50, 100, 127],\n",
        "            'subsample': [0.6, 0.8, 1.0],\n",
        "            'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "            'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
        "            'reg_lambda': [0, 0.1, 0.5, 1.0]\n",
        "        }\n",
        "\n",
        "        from itertools import product\n",
        "        import random\n",
        "\n",
        "        keys = list(param_grid.keys())\n",
        "        combinations = list(product(*[param_grid[key] for key in keys]))\n",
        "\n",
        "        # Limit to 50 combinations\n",
        "        if len(combinations) > 50:\n",
        "            combinations = random.sample(combinations, 50)\n",
        "\n",
        "        for target in self.target_columns:\n",
        "            print(f\"\\n  Grid Search for LightGBM - {target}...\")\n",
        "            print(f\"  Testing {len(combinations)} parameter combinations...\")\n",
        "\n",
        "            y_train = train_df[target].fillna(train_df[target].mean())\n",
        "            y_val = val_df[target].fillna(val_df[target].mean())\n",
        "            y_test = test_df[target].fillna(test_df[target].mean())\n",
        "\n",
        "            best_mae = float('inf')\n",
        "            best_params = None\n",
        "            best_model = None\n",
        "\n",
        "            for i, combo in enumerate(combinations):\n",
        "                if i % 10 == 0:\n",
        "                    print(f\"    Progress: {i}/{len(combinations)}\")\n",
        "\n",
        "                params = dict(zip(keys, combo))\n",
        "                params.update({\n",
        "                    'objective': 'regression',\n",
        "                    'random_state': 42,\n",
        "                    'verbose': -1\n",
        "                })\n",
        "\n",
        "                try:\n",
        "                    model = lgb.LGBMRegressor(**params)\n",
        "                    model.fit(X_train, y_train,\n",
        "                             eval_set=[(X_val, y_val)],\n",
        "                             callbacks=[lgb.early_stopping(20, verbose=False)])\n",
        "\n",
        "                    # Predict and aggregate\n",
        "                    val_pred = model.predict(X_val)\n",
        "                    val_actual_agg, val_pred_agg = self.aggregate_predictions_by_date(val_df, val_pred, target)\n",
        "\n",
        "                    val_metrics = self.calculate_metrics(val_actual_agg, val_pred_agg, 'LightGBM_GridSearch', target)\n",
        "\n",
        "                    if val_metrics['MAE'] < best_mae:\n",
        "                        best_mae = val_metrics['MAE']\n",
        "                        best_params = params.copy()\n",
        "                        best_model = model\n",
        "\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "            # Store best model\n",
        "            if best_model is not None:\n",
        "                test_pred = best_model.predict(X_test)\n",
        "                val_pred = best_model.predict(X_val)\n",
        "\n",
        "                val_actual_agg, val_pred_agg = self.aggregate_predictions_by_date(val_df, val_pred, target)\n",
        "                test_actual_agg, test_pred_agg = self.aggregate_predictions_by_date(test_df, test_pred, target)\n",
        "\n",
        "                val_metrics = self.calculate_metrics(val_actual_agg, val_pred_agg, 'LightGBM_GridSearch', target)\n",
        "                test_metrics = self.calculate_metrics(test_actual_agg, test_pred_agg, 'LightGBM_GridSearch', target)\n",
        "\n",
        "                model_key = f'LightGBM_GridSearch_{target}'\n",
        "                self.models[model_key] = best_model\n",
        "\n",
        "                joblib.dump(best_model, f'models/{model_key}.pkl')\n",
        "\n",
        "                self.results[model_key] = {\n",
        "                    'validation': val_metrics,\n",
        "                    'test': test_metrics,\n",
        "                    'best_params': best_params\n",
        "                }\n",
        "\n",
        "                print(f\"    Best MAE: {best_mae:.4f}\")\n",
        "\n",
        "    def train_lightgbm_models(self, train_df, val_df, test_df):\n",
        "        \"\"\"Train LightGBM models with default parameters\"\"\"\n",
        "        print(\"\\nTraining LightGBM models with default parameters...\")\n",
        "\n",
        "        X_train = train_df[self.feature_columns].fillna(0)\n",
        "        X_val = val_df[self.feature_columns].fillna(0)\n",
        "        X_test = test_df[self.feature_columns].fillna(0)\n",
        "\n",
        "        for target in self.target_columns:\n",
        "            print(f\"  Training LightGBM for {target}...\")\n",
        "\n",
        "            y_train = train_df[target].fillna(train_df[target].mean())\n",
        "            y_val = val_df[target].fillna(val_df[target].mean())\n",
        "            y_test = test_df[target].fillna(test_df[target].mean())\n",
        "\n",
        "            # LightGBM with default parameters\n",
        "            model = lgb.LGBMRegressor(\n",
        "                objective='regression',\n",
        "                random_state=42,\n",
        "                verbose=-1\n",
        "            )\n",
        "\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Predictions\n",
        "            val_pred = model.predict(X_val)\n",
        "            test_pred = model.predict(X_test)\n",
        "\n",
        "            # Aggregate by date\n",
        "            val_actual_agg, val_pred_agg = self.aggregate_predictions_by_date(val_df, val_pred, target)\n",
        "            test_actual_agg, test_pred_agg = self.aggregate_predictions_by_date(test_df, test_pred, target)\n",
        "\n",
        "            # Store model and results\n",
        "            model_key = f'LightGBM_Default_{target}'\n",
        "            self.models[model_key] = model\n",
        "\n",
        "            joblib.dump(model, f'models/{model_key}.pkl')\n",
        "\n",
        "            val_metrics = self.calculate_metrics(val_actual_agg, val_pred_agg, 'LightGBM_Default', target)\n",
        "            test_metrics = self.calculate_metrics(test_actual_agg, test_pred_agg, 'LightGBM_Default', target)\n",
        "\n",
        "            self.results[model_key] = {\n",
        "                'validation': val_metrics,\n",
        "                'test': test_metrics\n",
        "            }\n",
        "\n",
        "    def train_traditional_ml_grid_search(self, train_df, val_df, test_df):\n",
        "        \"\"\"Train traditional ML models with hyperparameter grid search (NO LINEAR MODELS)\"\"\"\n",
        "        print(\"\\nTraining Traditional ML models with Grid Search...\")\n",
        "\n",
        "        X_train = train_df[self.feature_columns].fillna(0)\n",
        "        X_val = val_df[self.feature_columns].fillna(0)\n",
        "        X_test = test_df[self.feature_columns].fillna(0)\n",
        "\n",
        "        # Define hyperparameter grids - REMOVED LINEAR MODELS\n",
        "        ml_param_grids = {\n",
        "            'RandomForest': {\n",
        "                'n_estimators': [100, 200, 300, 500],\n",
        "                'max_depth': [3, 5, 10, 15, None],\n",
        "                'min_samples_split': [2, 5, 10],\n",
        "                'min_samples_leaf': [1, 2, 4],\n",
        "                'max_features': ['sqrt', 'log2', 0.5, 0.8]\n",
        "            },\n",
        "            'GradientBoosting': {\n",
        "                'n_estimators': [100, 200, 300],\n",
        "                'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "                'max_depth': [3, 4, 5, 6],\n",
        "                'subsample': [0.6, 0.8, 1.0],\n",
        "                'min_samples_split': [2, 5, 10]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        from itertools import product\n",
        "        import random\n",
        "\n",
        "        for model_name, param_grid in ml_param_grids.items():\n",
        "            print(f\"\\n  Grid Search for {model_name}...\")\n",
        "\n",
        "            keys = list(param_grid.keys())\n",
        "            combinations = list(product(*[param_grid[key] for key in keys]))\n",
        "\n",
        "            if len(combinations) > 30:\n",
        "                combinations = random.sample(combinations, 30)\n",
        "\n",
        "            for target in self.target_columns:\n",
        "                print(f\"    {model_name} for {target}...\")\n",
        "\n",
        "                y_train = train_df[target].fillna(train_df[target].mean())\n",
        "                y_val = val_df[target].fillna(val_df[target].mean())\n",
        "                y_test = test_df[target].fillna(test_df[target].mean())\n",
        "\n",
        "                best_mae = float('inf')\n",
        "                best_params = None\n",
        "                best_model = None\n",
        "\n",
        "                for combo in combinations:\n",
        "                    params = dict(zip(keys, combo))\n",
        "                    params['random_state'] = 42\n",
        "\n",
        "                    try:\n",
        "                        if model_name == 'RandomForest':\n",
        "                            model = RandomForestRegressor(**params)\n",
        "                        elif model_name == 'GradientBoosting':\n",
        "                            model = GradientBoostingRegressor(**params)\n",
        "\n",
        "                        model.fit(X_train, y_train)\n",
        "\n",
        "                        # Predict and aggregate\n",
        "                        val_pred = model.predict(X_val)\n",
        "                        val_actual_agg, val_pred_agg = self.aggregate_predictions_by_date(val_df, val_pred, target)\n",
        "                        val_metrics = self.calculate_metrics(val_actual_agg, val_pred_agg, model_name, target)\n",
        "\n",
        "                        if val_metrics['MAE'] < best_mae:\n",
        "                            best_mae = val_metrics['MAE']\n",
        "                            best_params = params.copy()\n",
        "                            best_model = model\n",
        "\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "                # Store best model\n",
        "                if best_model is not None:\n",
        "                    test_pred = best_model.predict(X_test)\n",
        "                    val_pred = best_model.predict(X_val)\n",
        "\n",
        "                    val_actual_agg, val_pred_agg = self.aggregate_predictions_by_date(val_df, val_pred, target)\n",
        "                    test_actual_agg, test_pred_agg = self.aggregate_predictions_by_date(test_df, test_pred, target)\n",
        "\n",
        "                    val_metrics = self.calculate_metrics(val_actual_agg, val_pred_agg, model_name, target)\n",
        "                    test_metrics = self.calculate_metrics(test_actual_agg, test_pred_agg, model_name, target)\n",
        "\n",
        "                    model_key = f'{model_name}_GridSearch_{target}'\n",
        "                    self.models[model_key] = best_model\n",
        "\n",
        "                    joblib.dump(best_model, f'models/{model_key}.pkl')\n",
        "\n",
        "                    self.results[model_key] = {\n",
        "                        'validation': val_metrics,\n",
        "                        'test': test_metrics,\n",
        "                        'best_params': best_params\n",
        "                    }\n",
        "\n",
        "                    print(f\"      Best MAE: {best_mae:.4f}\")\n",
        "\n",
        "    def train_traditional_ml_models(self, train_df, val_df, test_df):\n",
        "        \"\"\"Train traditional ML models with default parameters (NO LINEAR MODELS)\"\"\"\n",
        "        print(\"\\nTraining traditional ML models with default parameters...\")\n",
        "\n",
        "        X_train = train_df[self.feature_columns].fillna(0)\n",
        "        X_val = val_df[self.feature_columns].fillna(0)\n",
        "        X_test = test_df[self.feature_columns].fillna(0)\n",
        "\n",
        "        # Define models - REMOVED LINEAR MODELS\n",
        "        ml_models = {\n",
        "            'RandomForest_Default': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10),\n",
        "            'GradientBoosting_Default': GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=6)\n",
        "        }\n",
        "\n",
        "        for model_name, model in ml_models.items():\n",
        "            for target in self.target_columns:\n",
        "                print(f\"  Training {model_name} for {target}...\")\n",
        "\n",
        "                y_train = train_df[target].fillna(train_df[target].mean())\n",
        "                y_val = val_df[target].fillna(val_df[target].mean())\n",
        "                y_test = test_df[target].fillna(test_df[target].mean())\n",
        "\n",
        "                # Train model\n",
        "                model.fit(X_train, y_train)\n",
        "\n",
        "                # Predictions\n",
        "                val_pred = model.predict(X_val)\n",
        "                test_pred = model.predict(X_test)\n",
        "\n",
        "                # Aggregate by date\n",
        "                val_actual_agg, val_pred_agg = self.aggregate_predictions_by_date(val_df, val_pred, target)\n",
        "                test_actual_agg, test_pred_agg = self.aggregate_predictions_by_date(test_df, test_pred, target)\n",
        "\n",
        "                # Store model and results\n",
        "                model_key = f'{model_name}_{target}'\n",
        "                self.models[model_key] = model\n",
        "\n",
        "                joblib.dump(model, f'models/{model_key}.pkl')\n",
        "\n",
        "                val_metrics = self.calculate_metrics(val_actual_agg, val_pred_agg, model_name, target)\n",
        "                test_metrics = self.calculate_metrics(test_actual_agg, test_pred_agg, model_name, target)\n",
        "\n",
        "                self.results[model_key] = {\n",
        "                    'validation': val_metrics,\n",
        "                    'test': test_metrics\n",
        "                }\n",
        "\n",
        "    def prepare_lstm_data(self, df, target_col, sequence_length=7):\n",
        "        \"\"\"Prepare data for LSTM/GRU models - aggregate by date first\"\"\"\n",
        "        # Aggregate by date\n",
        "        agg_df = df.groupby('date').agg({\n",
        "            target_col: 'mean'\n",
        "        }).reset_index()\n",
        "        agg_df = agg_df.sort_values('date')\n",
        "\n",
        "        # Scale data\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_data = scaler.fit_transform(agg_df[target_col].values.reshape(-1, 1))\n",
        "\n",
        "        # Create sequences\n",
        "        sequences = []\n",
        "        targets = []\n",
        "\n",
        "        for i in range(sequence_length, len(scaled_data)):\n",
        "            sequences.append(scaled_data[i-sequence_length:i])\n",
        "            targets.append(scaled_data[i])\n",
        "\n",
        "        return np.array(sequences), np.array(targets), scaler\n",
        "\n",
        "    def train_deep_learning_grid_search(self, train_df, val_df, test_df):\n",
        "        \"\"\"Train deep learning models with hyperparameter grid search\"\"\"\n",
        "        try:\n",
        "            print(\"\\nTraining Deep Learning models with Grid Search...\")\n",
        "\n",
        "            dl_param_grids = {\n",
        "                'LSTM': {\n",
        "                    'units': [32, 50, 64],\n",
        "                    'dropout': [0.1, 0.2],\n",
        "                    'batch_size': [16, 32],\n",
        "                    'learning_rate': [0.001, 0.01]\n",
        "                },\n",
        "                'GRU': {\n",
        "                    'units': [32, 50, 64],\n",
        "                    'dropout': [0.1, 0.2],\n",
        "                    'batch_size': [16, 32],\n",
        "                    'learning_rate': [0.001, 0.01]\n",
        "                }\n",
        "            }\n",
        "\n",
        "            from itertools import product\n",
        "            import random\n",
        "\n",
        "            for target in self.target_columns:\n",
        "                print(f\"  Preparing sequences for {target}...\")\n",
        "\n",
        "                # Prepare sequences with aggregation\n",
        "                X_train, y_train, train_scaler = self.prepare_lstm_data(train_df, target)\n",
        "                X_val, y_val, val_scaler = self.prepare_lstm_data(val_df, target)\n",
        "                X_test, y_test, test_scaler = self.prepare_lstm_data(test_df, target)\n",
        "\n",
        "                if len(X_train) < 10 or len(X_val) < 5:\n",
        "                    print(f\"    Insufficient data for {target}\")\n",
        "                    continue\n",
        "\n",
        "                for model_type in ['LSTM', 'GRU']:\n",
        "                    print(f\"    Grid Search for {model_type} - {target}...\")\n",
        "\n",
        "                    param_grid = dl_param_grids[model_type]\n",
        "                    keys = list(param_grid.keys())\n",
        "                    combinations = list(product(*[param_grid[key] for key in keys]))\n",
        "\n",
        "                    if len(combinations) > 15:\n",
        "                        combinations = random.sample(combinations, 15)\n",
        "\n",
        "                    best_mae = float('inf')\n",
        "                    best_params = None\n",
        "                    best_model = None\n",
        "\n",
        "                    for combo in combinations:\n",
        "                        params = dict(zip(keys, combo))\n",
        "\n",
        "                        try:\n",
        "                            model = Sequential()\n",
        "\n",
        "                            if model_type == 'LSTM':\n",
        "                                model.add(LSTM(params['units'], input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "                            else:  # GRU\n",
        "                                model.add(GRU(params['units'], input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "\n",
        "                            model.add(Dropout(params['dropout']))\n",
        "                            model.add(Dense(1))\n",
        "\n",
        "                            optimizer = Adam(learning_rate=params['learning_rate'])\n",
        "                            model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "                            early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "                            model.fit(X_train, y_train,\n",
        "                                    validation_data=(X_val, y_val),\n",
        "                                    epochs=30, batch_size=params['batch_size'],\n",
        "                                    callbacks=[early_stop], verbose=0)\n",
        "\n",
        "                            # Evaluate\n",
        "                            val_pred_scaled = model.predict(X_val, verbose=0)\n",
        "                            val_pred = val_scaler.inverse_transform(val_pred_scaled).flatten()\n",
        "                            val_actual = val_scaler.inverse_transform(y_val).flatten()\n",
        "\n",
        "                            val_metrics = self.calculate_metrics(val_actual, val_pred, model_type, target)\n",
        "\n",
        "                            if val_metrics['MAE'] < best_mae:\n",
        "                                best_mae = val_metrics['MAE']\n",
        "                                best_params = params.copy()\n",
        "                                best_model = model\n",
        "                                best_scaler = val_scaler\n",
        "\n",
        "                        except Exception as e:\n",
        "                            continue\n",
        "\n",
        "                    # Store best model\n",
        "                    if best_model is not None:\n",
        "                        test_pred_scaled = best_model.predict(X_test, verbose=0)\n",
        "                        test_pred = test_scaler.inverse_transform(test_pred_scaled).flatten()\n",
        "                        test_actual = test_scaler.inverse_transform(y_test).flatten()\n",
        "\n",
        "                        val_pred_scaled = best_model.predict(X_val, verbose=0)\n",
        "                        val_pred = val_scaler.inverse_transform(val_pred_scaled).flatten()\n",
        "                        val_actual = val_scaler.inverse_transform(y_val).flatten()\n",
        "\n",
        "                        val_metrics = self.calculate_metrics(val_actual, val_pred, model_type, target)\n",
        "                        test_metrics = self.calculate_metrics(test_actual, test_pred, model_type, target)\n",
        "\n",
        "                        model_key = f'{model_type}_GridSearch_{target}'\n",
        "                        self.models[model_key] = best_model\n",
        "\n",
        "                        best_model.save(f'models/{model_key}.h5')\n",
        "\n",
        "                        self.results[model_key] = {\n",
        "                            'validation': val_metrics,\n",
        "                            'test': test_metrics,\n",
        "                            'best_params': best_params\n",
        "                        }\n",
        "\n",
        "                        print(f\"      Best MAE: {best_mae:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in deep learning grid search: {e}\")\n",
        "\n",
        "    def train_deep_learning_models(self, train_df, val_df, test_df):\n",
        "        \"\"\"Train LSTM and GRU models with default parameters\"\"\"\n",
        "        try:\n",
        "            print(\"\\nTraining Deep Learning models...\")\n",
        "\n",
        "            for target in self.target_columns:\n",
        "                print(f\"  Preparing data for {target}...\")\n",
        "\n",
        "                # Prepare sequences with aggregation\n",
        "                X_train, y_train, train_scaler = self.prepare_lstm_data(train_df, target)\n",
        "                X_val, y_val, val_scaler = self.prepare_lstm_data(val_df, target)\n",
        "                X_test, y_test, test_scaler = self.prepare_lstm_data(test_df, target)\n",
        "\n",
        "                if len(X_train) < 10 or len(X_val) < 5 or len(X_test) < 5:\n",
        "                    print(f\"    Insufficient data for {target}, skipping deep learning models\")\n",
        "                    continue\n",
        "\n",
        "                # LSTM Model\n",
        "                print(f\"    Training LSTM for {target}...\")\n",
        "                lstm_model = Sequential([\n",
        "                    LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "                    Dropout(0.2),\n",
        "                    Dense(1)\n",
        "                ])\n",
        "\n",
        "                lstm_model.compile(optimizer=Adam(), loss='mse')\n",
        "\n",
        "                early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "                lstm_model.fit(X_train, y_train,\n",
        "                              validation_data=(X_val, y_val),\n",
        "                              epochs=50, batch_size=16,\n",
        "                              callbacks=[early_stop], verbose=0)\n",
        "\n",
        "                # LSTM Predictions\n",
        "                val_pred_scaled = lstm_model.predict(X_val, verbose=0)\n",
        "                test_pred_scaled = lstm_model.predict(X_test, verbose=0)\n",
        "\n",
        "                val_pred = val_scaler.inverse_transform(val_pred_scaled).flatten()\n",
        "                test_pred = test_scaler.inverse_transform(test_pred_scaled).flatten()\n",
        "                val_actual = val_scaler.inverse_transform(y_val).flatten()\n",
        "                test_actual = test_scaler.inverse_transform(y_test).flatten()\n",
        "\n",
        "                # Store LSTM results\n",
        "                model_key = f'LSTM_{target}'\n",
        "                self.models[model_key] = lstm_model\n",
        "                lstm_model.save(f'models/{model_key}.h5')\n",
        "\n",
        "                val_metrics = self.calculate_metrics(val_actual, val_pred, 'LSTM', target)\n",
        "                test_metrics = self.calculate_metrics(test_actual, test_pred, 'LSTM', target)\n",
        "\n",
        "                self.results[model_key] = {\n",
        "                    'validation': val_metrics,\n",
        "                    'test': test_metrics\n",
        "                }\n",
        "\n",
        "                # GRU Model\n",
        "                print(f\"    Training GRU for {target}...\")\n",
        "                gru_model = Sequential([\n",
        "                    GRU(50, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "                    Dropout(0.2),\n",
        "                    Dense(1)\n",
        "                ])\n",
        "\n",
        "                gru_model.compile(optimizer=Adam(), loss='mse')\n",
        "                gru_model.fit(X_train, y_train,\n",
        "                             validation_data=(X_val, y_val),\n",
        "                             epochs=50, batch_size=16,\n",
        "                             callbacks=[early_stop], verbose=0)\n",
        "\n",
        "                # GRU Predictions\n",
        "                val_pred_scaled = gru_model.predict(X_val, verbose=0)\n",
        "                test_pred_scaled = gru_model.predict(X_test, verbose=0)\n",
        "\n",
        "                val_pred = val_scaler.inverse_transform(val_pred_scaled).flatten()\n",
        "                test_pred = test_scaler.inverse_transform(test_pred_scaled).flatten()\n",
        "                val_actual = val_scaler.inverse_transform(y_val).flatten()\n",
        "                test_actual = test_scaler.inverse_transform(y_test).flatten()\n",
        "\n",
        "                # Store GRU results\n",
        "                model_key = f'GRU_{target}'\n",
        "                self.models[model_key] = gru_model\n",
        "                gru_model.save(f'models/{model_key}.h5')\n",
        "\n",
        "                val_metrics = self.calculate_metrics(val_actual, val_pred, 'GRU', target)\n",
        "                test_metrics = self.calculate_metrics(test_actual, test_pred, 'GRU', target)\n",
        "\n",
        "                self.results[model_key] = {\n",
        "                    'validation': val_metrics,\n",
        "                    'test': test_metrics\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in deep learning training: {e}\")\n",
        "\n",
        "    def train_time_series_models(self, train_df, val_df, test_df):\n",
        "        \"\"\"Train ARIMA and SARIMA models - aggregate by date approach\"\"\"\n",
        "        try:\n",
        "            print(\"\\nTraining Time Series models...\")\n",
        "\n",
        "            for target in self.target_columns:\n",
        "                print(f\"  Training ARIMA/SARIMA for {target}...\")\n",
        "\n",
        "                # Aggregate by date\n",
        "                train_ts = train_df.groupby('date')[target].mean().reset_index()\n",
        "                val_ts = val_df.groupby('date')[target].mean().reset_index()\n",
        "                test_ts = test_df.groupby('date')[target].mean().reset_index()\n",
        "\n",
        "                train_ts['date'] = pd.to_datetime(train_ts['date'])\n",
        "                val_ts['date'] = pd.to_datetime(val_ts['date'])\n",
        "                test_ts['date'] = pd.to_datetime(test_ts['date'])\n",
        "\n",
        "                train_ts = train_ts.sort_values('date').set_index('date')[target]\n",
        "                val_ts = val_ts.sort_values('date').set_index('date')[target]\n",
        "                test_ts = test_ts.sort_values('date').set_index('date')[target]\n",
        "\n",
        "                if len(train_ts) < 20:\n",
        "                    print(f\"    Insufficient data for ARIMA {target}, skipping...\")\n",
        "                    continue\n",
        "\n",
        "                # ARIMA - Try different orders\n",
        "                print(f\"    Fitting ARIMA for {target}...\")\n",
        "                arima_orders = [(1,1,1), (2,1,1), (1,1,2), (2,1,2), (0,1,1), (1,0,1)]\n",
        "                best_aic = float('inf')\n",
        "                best_arima = None\n",
        "                best_arima_order = None\n",
        "\n",
        "                for order in arima_orders:\n",
        "                    try:\n",
        "                        arima_model = ARIMA(train_ts, order=order)\n",
        "                        arima_fitted = arima_model.fit()\n",
        "\n",
        "                        if arima_fitted.aic < best_aic:\n",
        "                            best_aic = arima_fitted.aic\n",
        "                            best_arima = arima_fitted\n",
        "                            best_arima_order = order\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                if best_arima is not None:\n",
        "                    try:\n",
        "                        # Forecast\n",
        "                        val_forecast = best_arima.forecast(steps=len(val_ts))\n",
        "                        test_forecast = best_arima.forecast(steps=len(test_ts))\n",
        "\n",
        "                        # Store model\n",
        "                        model_key = f'ARIMA_{target}'\n",
        "                        self.models[model_key] = best_arima\n",
        "\n",
        "                        with open(f'models/{model_key}.pkl', 'wb') as f:\n",
        "                            pickle.dump(best_arima, f)\n",
        "\n",
        "                        # Calculate metrics\n",
        "                        val_metrics = self.calculate_metrics(val_ts.values, val_forecast, 'ARIMA', target)\n",
        "                        test_metrics = self.calculate_metrics(test_ts.values, test_forecast, 'ARIMA', target)\n",
        "\n",
        "                        self.results[model_key] = {\n",
        "                            'validation': val_metrics,\n",
        "                            'test': test_metrics,\n",
        "                            'best_params': {'order': best_arima_order, 'aic': best_aic}\n",
        "                        }\n",
        "\n",
        "                        print(f\"    ARIMA - Best order: {best_arima_order}, AIC: {best_aic:.2f}\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"    ARIMA forecasting failed: {e}\")\n",
        "\n",
        "                # SARIMA\n",
        "                print(f\"    Fitting SARIMA for {target}...\")\n",
        "                try:\n",
        "                    sarima_model = SARIMAX(train_ts, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7))\n",
        "                    sarima_fitted = sarima_model.fit(disp=False)\n",
        "\n",
        "                    # Forecast\n",
        "                    val_forecast = sarima_fitted.forecast(steps=len(val_ts))\n",
        "                    test_forecast = sarima_fitted.forecast(steps=len(test_ts))\n",
        "\n",
        "                    # Store model\n",
        "                    model_key = f'SARIMA_{target}'\n",
        "                    self.models[model_key] = sarima_fitted\n",
        "\n",
        "                    with open(f'models/{model_key}.pkl', 'wb') as f:\n",
        "                        pickle.dump(sarima_fitted, f)\n",
        "\n",
        "                    # Calculate metrics\n",
        "                    val_metrics = self.calculate_metrics(val_ts.values, val_forecast, 'SARIMA', target)\n",
        "                    test_metrics = self.calculate_metrics(test_ts.values, test_forecast, 'SARIMA', target)\n",
        "\n",
        "                    self.results[model_key] = {\n",
        "                        'validation': val_metrics,\n",
        "                        'test': test_metrics,\n",
        "                        'best_params': {'order': (1,1,1), 'seasonal_order': (1,1,1,7)}\n",
        "                    }\n",
        "\n",
        "                    print(f\"    SARIMA - Training complete\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"    SARIMA training failed: {e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in time series training: {e}\")\n",
        "\n",
        "    def create_comparison_table(self):\n",
        "        \"\"\"Create comparison table of all models\"\"\"\n",
        "        print(\"\\nCreating comparison table...\")\n",
        "\n",
        "        comparison_data = []\n",
        "\n",
        "        for model_key, metrics in self.results.items():\n",
        "            # Parse model name and target\n",
        "            if model_key.count('_') >= 2:\n",
        "                parts = model_key.split('_')\n",
        "                if len(parts) >= 3:\n",
        "                    target = None\n",
        "                    for i in range(len(parts)-1, 0, -1):\n",
        "                        potential_target = '_'.join(parts[i:])\n",
        "                        if potential_target in self.target_columns:\n",
        "                            target = potential_target\n",
        "                            model_name = '_'.join(parts[:i])\n",
        "                            break\n",
        "\n",
        "                    if target is None:\n",
        "                        model_name, target = model_key.rsplit('_', 1)\n",
        "                else:\n",
        "                    model_name, target = model_key.rsplit('_', 1)\n",
        "            else:\n",
        "                model_name, target = model_key.rsplit('_', 1)\n",
        "\n",
        "            if 'validation' in metrics and 'test' in metrics:\n",
        "                row = {\n",
        "                    'Model': model_name,\n",
        "                    'Target': target,\n",
        "                    'Val_MAE': metrics['validation']['MAE'],\n",
        "                    'Val_RMSE': metrics['validation']['RMSE'],\n",
        "                    'Val_MAPE': metrics['validation']['MAPE'],\n",
        "                    'Val_Bias': metrics['validation']['Bias'],\n",
        "                    'Test_MAE': metrics['test']['MAE'],\n",
        "                    'Test_RMSE': metrics['test']['RMSE'],\n",
        "                    'Test_MAPE': metrics['test']['MAPE'],\n",
        "                    'Test_Bias': metrics['test']['Bias']\n",
        "                }\n",
        "\n",
        "                if 'best_params' in metrics:\n",
        "                    params_str = str(metrics['best_params'])[:100] + \"...\" if len(str(metrics['best_params'])) > 100 else str(metrics['best_params'])\n",
        "                    row['Hyperparameters'] = params_str\n",
        "                else:\n",
        "                    row['Hyperparameters'] = 'Default'\n",
        "\n",
        "                comparison_data.append(row)\n",
        "\n",
        "        comparison_df = pd.DataFrame(comparison_data)\n",
        "        comparison_df = comparison_df.round(4)\n",
        "\n",
        "        # Save comparison table\n",
        "        comparison_df.to_csv('results/model_comparison.csv', index=False)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"MODEL COMPARISON TABLE\")\n",
        "        print(\"=\"*80)\n",
        "        print(comparison_df.to_string(index=False))\n",
        "\n",
        "        return comparison_df\n",
        "\n",
        "    def get_top_models_summary(self, comparison_df):\n",
        "        \"\"\"Get top 1 model for each target with detailed info\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"TOP 1 MODEL FOR EACH TARGET\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        top_models_data = []\n",
        "\n",
        "        for target in self.target_columns:\n",
        "            target_results = comparison_df[comparison_df['Target'] == target].copy()\n",
        "\n",
        "            if len(target_results) > 0:\n",
        "                # Find best model based on validation MAE\n",
        "                best_idx = target_results['Val_MAE'].idxmin()\n",
        "                best_row = target_results.loc[best_idx]\n",
        "\n",
        "                model_name = best_row['Model']\n",
        "                model_key = f\"{model_name}_{target}\"\n",
        "\n",
        "                # Get parameters if available\n",
        "                params = \"Default\"\n",
        "                if 'Hyperparameters' in best_row and pd.notna(best_row['Hyperparameters']):\n",
        "                    params = best_row['Hyperparameters']\n",
        "                elif model_key in self.results and 'best_params' in self.results[model_key]:\n",
        "                    params = str(self.results[model_key]['best_params'])\n",
        "                elif target in self.hyperparameter_results:\n",
        "                    params = str(self.hyperparameter_results[target]['best_params'])\n",
        "\n",
        "                top_model_info = {\n",
        "                    'Target': target,\n",
        "                    'Best_Model': model_name,\n",
        "                    'Val_MAE': best_row['Val_MAE'],\n",
        "                    'Val_RMSE': best_row['Val_RMSE'],\n",
        "                    'Val_MAPE': best_row['Val_MAPE'],\n",
        "                    'Val_Bias': best_row['Val_Bias'],\n",
        "                    'Test_MAE': best_row['Test_MAE'],\n",
        "                    'Test_RMSE': best_row['Test_RMSE'],\n",
        "                    'Test_MAPE': best_row['Test_MAPE'],\n",
        "                    'Test_Bias': best_row['Test_Bias'],\n",
        "                    'Hyperparameters': params\n",
        "                }\n",
        "\n",
        "                top_models_data.append(top_model_info)\n",
        "\n",
        "                # Copy best model to top_models directory\n",
        "                try:\n",
        "                    import shutil\n",
        "                    for ext in ['.pkl', '.h5']:\n",
        "                        source_file = f'models/{model_key}{ext}'\n",
        "                        if os.path.exists(source_file):\n",
        "                            dest_file = f'top_models/{target}_best_model{ext}'\n",
        "                            shutil.copy2(source_file, dest_file)\n",
        "                            print(f\"✅ Moved {model_name} for {target} to top_models/\")\n",
        "                            break\n",
        "                    else:\n",
        "                        print(f\"⚠️  Model file not found for {model_key}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️  Could not copy model for {target}: {e}\")\n",
        "\n",
        "        # Create DataFrame and save\n",
        "        if top_models_data:\n",
        "            top_models_df = pd.DataFrame(top_models_data)\n",
        "            top_models_df = top_models_df.round(4)\n",
        "\n",
        "            top_models_df.to_csv('results/top_models_summary.csv', index=False)\n",
        "\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"TOP MODELS SUMMARY\")\n",
        "            print(\"=\"*60)\n",
        "            print(top_models_df.to_string(index=False))\n",
        "        else:\n",
        "            print(\"No top models found!\")\n",
        "            top_models_df = pd.DataFrame()\n",
        "\n",
        "        return top_models_df\n",
        "\n",
        "    def run_complete_pipeline(self):\n",
        "        \"\"\"Run the complete ML pipeline\"\"\"\n",
        "        print(\"=\"*80)\n",
        "        print(\"STARTING COMPLETE ML FORECASTING PIPELINE\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Load data\n",
        "        df = self.load_and_prepare_data()\n",
        "\n",
        "        # Create splits\n",
        "        train_df, val_df, test_df = self.create_train_val_test_split(df)\n",
        "\n",
        "        # Train all models\n",
        "        self.train_xgboost_grid_search(train_df, val_df, test_df)\n",
        "        self.train_xgboost_models(train_df, val_df, test_df)\n",
        "        self.train_lightgbm_grid_search(train_df, val_df, test_df)\n",
        "        self.train_lightgbm_models(train_df, val_df, test_df)\n",
        "        self.train_traditional_ml_grid_search(train_df, val_df, test_df)\n",
        "        self.train_traditional_ml_models(train_df, val_df, test_df)\n",
        "        self.train_deep_learning_grid_search(train_df, val_df, test_df)\n",
        "        self.train_deep_learning_models(train_df, val_df, test_df)\n",
        "        self.train_time_series_models(train_df, val_df, test_df)\n",
        "\n",
        "        # Create comparison table\n",
        "        comparison_df = self.create_comparison_table()\n",
        "\n",
        "        # Get top models summary\n",
        "        top_models_df = self.get_top_models_summary(comparison_df)\n",
        "\n",
        "        print(f\"\\n✅ Pipeline complete! All models saved in 'models/' directory\")\n",
        "        print(f\"✅ Top models moved to 'top_models/' directory\")\n",
        "        print(f\"✅ Results saved in 'results/' directory\")\n",
        "\n",
        "        return comparison_df, top_models_df\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# pipeline = ForecastingPipeline('data/processed/train.csv')\n",
        "# comparison_df, top_models_df = pipeline.run_complete_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = ForecastingPipeline('/content/enhanced_features.csv')\n",
        "comparison_df, top_models_df = pipeline.run_complete_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzrGtyVTsTWR",
        "outputId": "a3ca2fa0-664d-48eb-e4bf-972d7d7a27db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STARTING COMPLETE ML FORECASTING PIPELINE\n",
            "================================================================================\n",
            "Loading and preparing data...\n",
            "Data loaded: (1080, 52)\n",
            "Feature columns: 47\n",
            "Target columns: ['usage_cpu', 'usage_storage', 'users_active']\n",
            "Train: 744 samples\n",
            "Validation: 216 samples\n",
            "Test: 120 samples\n",
            "\n",
            "Training XGBoost with Grid Search (this may take a while)...\n",
            "\n",
            "  Grid Search for usage_cpu...\n",
            "  Testing 50 parameter combinations...\n",
            "    Progress: 0/50\n",
            "    Progress: 10/50\n",
            "    Progress: 20/50\n",
            "    Progress: 30/50\n",
            "    Progress: 40/50\n",
            "    Best MAE for usage_cpu: 2.8605\n",
            "    Best params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200, 'subsample': 0.9, 'colsample_bytree': 0.9, 'reg_alpha': 1.0, 'reg_lambda': 3.0, 'gamma': 0, 'min_child_weight': 7, 'objective': 'reg:squarederror', 'random_state': 42, 'early_stopping_rounds': 20}\n",
            "\n",
            "  Grid Search for usage_storage...\n",
            "  Testing 50 parameter combinations...\n",
            "    Progress: 0/50\n",
            "    Progress: 10/50\n",
            "    Progress: 20/50\n",
            "    Progress: 30/50\n",
            "    Progress: 40/50\n",
            "    Best MAE for usage_storage: 2.4067\n",
            "    Best params: {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 500, 'subsample': 0.8, 'colsample_bytree': 0.9, 'reg_alpha': 1.0, 'reg_lambda': 5.0, 'gamma': 0.1, 'min_child_weight': 7, 'objective': 'reg:squarederror', 'random_state': 42, 'early_stopping_rounds': 20}\n",
            "\n",
            "  Grid Search for users_active...\n",
            "  Testing 50 parameter combinations...\n",
            "    Progress: 0/50\n",
            "    Progress: 10/50\n",
            "    Progress: 20/50\n",
            "    Progress: 30/50\n",
            "    Progress: 40/50\n",
            "    Best MAE for users_active: 24.0366\n",
            "    Best params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 500, 'subsample': 0.6, 'colsample_bytree': 0.8, 'reg_alpha': 0.5, 'reg_lambda': 5.0, 'gamma': 0, 'min_child_weight': 1, 'objective': 'reg:squarederror', 'random_state': 42, 'early_stopping_rounds': 20}\n",
            "\n",
            "Training XGBoost models with default parameters...\n",
            "  Training XGBoost for usage_cpu...\n",
            "  Training XGBoost for usage_storage...\n",
            "  Training XGBoost for users_active...\n",
            "\n",
            "Training LightGBM with Grid Search...\n",
            "\n",
            "  Grid Search for LightGBM - usage_cpu...\n",
            "  Testing 50 parameter combinations...\n",
            "    Progress: 0/50\n",
            "    Progress: 10/50\n",
            "    Progress: 20/50\n",
            "    Progress: 30/50\n",
            "    Progress: 40/50\n",
            "    Best MAE: 2.8064\n",
            "\n",
            "  Grid Search for LightGBM - usage_storage...\n",
            "  Testing 50 parameter combinations...\n",
            "    Progress: 0/50\n",
            "    Progress: 10/50\n",
            "    Progress: 20/50\n",
            "    Progress: 30/50\n",
            "    Progress: 40/50\n",
            "    Best MAE: 1.0103\n",
            "\n",
            "  Grid Search for LightGBM - users_active...\n",
            "  Testing 50 parameter combinations...\n",
            "    Progress: 0/50\n",
            "    Progress: 10/50\n",
            "    Progress: 20/50\n",
            "    Progress: 30/50\n",
            "    Progress: 40/50\n",
            "    Best MAE: 24.5265\n",
            "\n",
            "Training LightGBM models with default parameters...\n",
            "  Training LightGBM for usage_cpu...\n",
            "  Training LightGBM for usage_storage...\n",
            "  Training LightGBM for users_active...\n",
            "\n",
            "Training Traditional ML models with Grid Search...\n",
            "\n",
            "  Grid Search for RandomForest...\n",
            "    RandomForest for usage_cpu...\n",
            "      Best MAE: 2.8819\n",
            "    RandomForest for usage_storage...\n",
            "      Best MAE: 1.7432\n",
            "    RandomForest for users_active...\n",
            "      Best MAE: 25.4456\n",
            "\n",
            "  Grid Search for GradientBoosting...\n",
            "    GradientBoosting for usage_cpu...\n",
            "      Best MAE: 2.7387\n",
            "    GradientBoosting for usage_storage...\n",
            "      Best MAE: 0.5440\n",
            "    GradientBoosting for users_active...\n",
            "      Best MAE: 25.4553\n",
            "\n",
            "Training traditional ML models with default parameters...\n",
            "  Training RandomForest_Default for usage_cpu...\n",
            "  Training RandomForest_Default for usage_storage...\n",
            "  Training RandomForest_Default for users_active...\n",
            "  Training GradientBoosting_Default for usage_cpu...\n",
            "  Training GradientBoosting_Default for usage_storage...\n",
            "  Training GradientBoosting_Default for users_active...\n",
            "\n",
            "Training Deep Learning models with Grid Search...\n",
            "  Preparing sequences for usage_cpu...\n",
            "    Grid Search for LSTM - usage_cpu...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79df80758360> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79df8015ba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Best MAE: 3.5999\n",
            "    Grid Search for GRU - usage_cpu...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Best MAE: 3.5211\n",
            "  Preparing sequences for usage_storage...\n",
            "    Grid Search for LSTM - usage_storage...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Best MAE: 81.4685\n",
            "    Grid Search for GRU - usage_storage...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Best MAE: 82.8246\n",
            "  Preparing sequences for users_active...\n",
            "    Grid Search for LSTM - users_active...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Best MAE: 34.0843\n",
            "    Grid Search for GRU - users_active...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Best MAE: 32.5961\n",
            "\n",
            "Training Deep Learning models...\n",
            "  Preparing data for usage_cpu...\n",
            "    Insufficient data for usage_cpu, skipping deep learning models\n",
            "  Preparing data for usage_storage...\n",
            "    Insufficient data for usage_storage, skipping deep learning models\n",
            "  Preparing data for users_active...\n",
            "    Insufficient data for users_active, skipping deep learning models\n",
            "\n",
            "Training Time Series models...\n",
            "  Training ARIMA/SARIMA for usage_cpu...\n",
            "    Fitting ARIMA for usage_cpu...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ARIMA - Best order: (2, 1, 1), AIC: 355.23\n",
            "    Fitting SARIMA for usage_cpu...\n",
            "    SARIMA - Training complete\n",
            "  Training ARIMA/SARIMA for usage_storage...\n",
            "    Fitting ARIMA for usage_storage...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ARIMA - Best order: (0, 1, 1), AIC: 777.95\n",
            "    Fitting SARIMA for usage_storage...\n",
            "    SARIMA - Training complete\n",
            "  Training ARIMA/SARIMA for users_active...\n",
            "    Fitting ARIMA for users_active...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ARIMA - Best order: (0, 1, 1), AIC: 560.01\n",
            "    Fitting SARIMA for users_active...\n",
            "    SARIMA - Training complete\n",
            "\n",
            "Creating comparison table...\n",
            "\n",
            "================================================================================\n",
            "MODEL COMPARISON TABLE\n",
            "================================================================================\n",
            "                      Model        Target  Val_MAE  Val_RMSE  Val_MAPE  Val_Bias  Test_MAE  Test_RMSE  Test_MAPE  Test_Bias                                                                                         Hyperparameters\n",
            "         XGBoost_GridSearch     usage_cpu   2.8605    3.6093    3.8862    0.7494    4.4276     4.8492     6.0059     0.9195 {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200, 'subsample': 0.9, 'colsample_bytree': 0....\n",
            "         XGBoost_GridSearch usage_storage   2.4067    2.9738    0.2016   -1.5095    3.6985     4.5832     0.2925    -1.3985 {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 500, 'subsample': 0.8, 'colsample_bytree': 0...\n",
            "         XGBoost_GridSearch  users_active  24.0366   30.3922    7.4034    5.6447   23.1678    25.1671     6.3983   -11.3608 {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 500, 'subsample': 0.6, 'colsample_bytree': 0....\n",
            "            XGBoost_Default     usage_cpu   2.9941    3.7139    4.0652    0.9128    4.0863     4.4842     5.5693     1.2842                                                                                                 Default\n",
            "            XGBoost_Default usage_storage  13.0748   16.4742    1.0603   -3.8308   17.8094    20.0514     1.4678     0.4958                                                                                                 Default\n",
            "            XGBoost_Default  users_active  24.8385   32.1254    7.6556    5.3627   23.9591    25.9476     6.6334   -11.3219                                                                                                 Default\n",
            "        LightGBM_GridSearch     usage_cpu   2.8064    3.5212    3.8112    0.7722    4.4754     4.8196     6.1095     1.4288 {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200, 'num_leaves': 127, 'subsample': 1.0, 'c...\n",
            "        LightGBM_GridSearch usage_storage   1.0103    1.2812    0.0833   -0.4817    1.5030     1.9235     0.1248     0.7014 {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 800, 'num_leaves': 31, 'subsample': 0.8, 'col...\n",
            "        LightGBM_GridSearch  users_active  24.5265   32.2235    7.5934    6.6195   22.8586    24.9644     6.3522    -9.6204 {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'num_leaves': 50, 'subsample': 0.8, 'col...\n",
            "           LightGBM_Default     usage_cpu   2.9485    4.0681    3.9510   -0.0942    5.3382     5.8222     7.2332     0.3737                                                                                                 Default\n",
            "           LightGBM_Default usage_storage   1.1763    1.5872    0.0957   -0.8659    1.3665     1.7423     0.1135     0.6648                                                                                                 Default\n",
            "           LightGBM_Default  users_active  30.0803   41.5199    9.4002   14.2612   15.3846    20.3997     4.4203     0.8515                                                                                                 Default\n",
            "    RandomForest_GridSearch     usage_cpu   2.8819    3.6832    3.9183    0.8601    4.4754     4.9168     6.0863     1.1541 {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features':...\n",
            "    RandomForest_GridSearch usage_storage   1.7432    2.2720    0.1405   -1.5346    2.4690     3.6767     0.1881    -1.8780 {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features'...\n",
            "    RandomForest_GridSearch  users_active  25.4456   34.1967    7.9543   11.3493   20.6438    23.6017     5.7899    -6.1082 {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features':...\n",
            "GradientBoosting_GridSearch     usage_cpu   2.7387    3.6283    3.6939    0.8166    5.8135     6.5139     7.9223     1.4115 {'n_estimators': 300, 'learning_rate': 0.1, 'max_depth': 6, 'subsample': 1.0, 'min_samples_split': 5...\n",
            "GradientBoosting_GridSearch usage_storage   0.5440    0.6377    0.0449   -0.2857    0.8500     1.1127     0.0691    -0.0472 {'n_estimators': 300, 'learning_rate': 0.1, 'max_depth': 6, 'subsample': 0.6, 'min_samples_split': 2...\n",
            "GradientBoosting_GridSearch  users_active  25.4553   33.7741    7.9332   10.4790   20.6241    23.4831     5.7689    -6.8377 {'n_estimators': 100, 'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.6, 'min_samples_split': ...\n",
            "       RandomForest_Default     usage_cpu   3.1935    4.0154    4.3371    0.7737    4.8263     5.5224     6.5555     0.9708                                                                                                 Default\n",
            "       RandomForest_Default usage_storage   1.0418    1.3387    0.0865   -0.7641    1.3542     1.5367     0.1081     0.0298                                                                                                 Default\n",
            "       RandomForest_Default  users_active  28.4424   37.2530    8.8459   11.6673   20.7594    23.1971     5.7979    -7.7081                                                                                                 Default\n",
            "   GradientBoosting_Default     usage_cpu   3.2426    4.4080    4.3646    0.5549    5.9633     6.6506     8.0954     0.8592                                                                                                 Default\n",
            "   GradientBoosting_Default usage_storage   0.7842    0.9061    0.0662   -0.0852    0.9735     1.1108     0.0798     0.5230                                                                                                 Default\n",
            "   GradientBoosting_Default  users_active  33.1701   42.2239   10.2214   13.2723   17.4691    23.6325     4.9231    -4.2446                                                                                                 Default\n",
            "            LSTM_GridSearch     usage_cpu   3.5999    4.1327    4.7604   -0.2309    6.4026     6.7233     8.4718    -1.2708                                  {'units': 32, 'dropout': 0.1, 'batch_size': 16, 'learning_rate': 0.01}\n",
            "             GRU_GridSearch     usage_cpu   3.5211    4.1359    4.5988   -1.1601    6.3778     6.5247     8.3718    -1.9643                                  {'units': 64, 'dropout': 0.1, 'batch_size': 16, 'learning_rate': 0.01}\n",
            "            LSTM_GridSearch usage_storage  81.4685   97.0251    6.7754   11.6422  109.7154   120.0102     9.8091    43.9271                                  {'units': 64, 'dropout': 0.1, 'batch_size': 16, 'learning_rate': 0.01}\n",
            "             GRU_GridSearch usage_storage  82.8246   95.2792    6.8457    4.0370  107.9185   122.1382     9.8581    86.4083                                 {'units': 50, 'dropout': 0.2, 'batch_size': 32, 'learning_rate': 0.001}\n",
            "            LSTM_GridSearch  users_active  34.0843   38.7640   10.3655   -0.1904   29.2816    31.6113     8.5415     1.4639                                  {'units': 50, 'dropout': 0.2, 'batch_size': 16, 'learning_rate': 0.01}\n",
            "             GRU_GridSearch  users_active  32.5961   38.7984   10.1148    5.9125   27.8990    31.6438     8.2246     4.7422                                  {'units': 50, 'dropout': 0.2, 'batch_size': 16, 'learning_rate': 0.01}\n",
            "                      ARIMA     usage_cpu   2.9382    3.7193    3.9183   -0.4287    3.9502     4.4679     5.3293     0.4521                                             {'order': (2, 1, 1), 'aic': np.float64(355.22888292716215)}\n",
            "                     SARIMA     usage_cpu   3.9422    4.6545    5.2581   -0.7667    4.7900     5.5029     6.4279     0.0542                                                    {'order': (1, 1, 1), 'seasonal_order': (1, 1, 1, 7)}\n",
            "                      ARIMA usage_storage  96.4888  109.5452    8.1655   43.5960   85.5250   105.2890     7.0260     2.8441                                              {'order': (0, 1, 1), 'aic': np.float64(777.9508256343604)}\n",
            "                     SARIMA usage_storage  90.0069  106.6694    7.6378   51.1367  100.4355   122.4559     8.2683     5.0305                                                    {'order': (1, 1, 1), 'seasonal_order': (1, 1, 1, 7)}\n",
            "                      ARIMA  users_active  25.8088   36.6219    8.2053   18.6719   17.8221    23.3844     5.1286     2.2562                                               {'order': (0, 1, 1), 'aic': np.float64(560.013597428531)}\n",
            "                     SARIMA  users_active  32.7652   44.3826   10.3200   25.6833   17.2471    25.7336     5.0828     5.1996                                                    {'order': (1, 1, 1), 'seasonal_order': (1, 1, 1, 7)}\n",
            "\n",
            "============================================================\n",
            "TOP 1 MODEL FOR EACH TARGET\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Moved GradientBoosting_GridSearch for usage_cpu to top_models/\n",
            "✅ Moved GradientBoosting_GridSearch for usage_storage to top_models/\n",
            "✅ Moved XGBoost_GridSearch for users_active to top_models/\n",
            "\n",
            "============================================================\n",
            "TOP MODELS SUMMARY\n",
            "============================================================\n",
            "       Target                  Best_Model  Val_MAE  Val_RMSE  Val_MAPE  Val_Bias  Test_MAE  Test_RMSE  Test_MAPE  Test_Bias                                                                                         Hyperparameters\n",
            "    usage_cpu GradientBoosting_GridSearch   2.7387    3.6283    3.6939    0.8166    5.8135     6.5139     7.9223     1.4115 {'n_estimators': 300, 'learning_rate': 0.1, 'max_depth': 6, 'subsample': 1.0, 'min_samples_split': 5...\n",
            "usage_storage GradientBoosting_GridSearch   0.5440    0.6377    0.0449   -0.2857    0.8500     1.1127     0.0691    -0.0472 {'n_estimators': 300, 'learning_rate': 0.1, 'max_depth': 6, 'subsample': 0.6, 'min_samples_split': 2...\n",
            " users_active          XGBoost_GridSearch  24.0366   30.3922    7.4034    5.6447   23.1678    25.1671     6.3983   -11.3608 {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 500, 'subsample': 0.6, 'colsample_bytree': 0....\n",
            "\n",
            "✅ Pipeline complete! All models saved in 'models/' directory\n",
            "✅ Top models moved to 'top_models/' directory\n",
            "✅ Results saved in 'results/' directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_cpu = joblib.load('/content/top_models/usage_cpu_best_model.pkl')\n",
        "model_storage = joblib.load('/content/top_models/usage_storage_best_model.pkl')\n",
        "model_users = joblib.load('/content/top_models/users_active_best_model.pkl')\n",
        "encoded_insights = pd.read_csv('/content/enhanced_features.csv')\n"
      ],
      "metadata": {
        "id": "LevQWMbf-gP_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forecast_next_30_days(model, target_col, df, variability_factor=0.25, seed=42):\n",
        "\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    df = df.copy()\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "    hist_df = df.copy()\n",
        "    last_date = hist_df[\"date\"].max()\n",
        "    forecasts = []\n",
        "\n",
        "    z_score = 1.96\n",
        "\n",
        "    for i in range(30):\n",
        "        next_date = last_date + timedelta(days=i + 1)\n",
        "        new_row = hist_df.iloc[-1:].copy()\n",
        "        new_row[\"date\"] = next_date\n",
        "        new_row[\"month\"] = next_date.month\n",
        "        new_row[\"dayofweek\"] = next_date.dayofweek\n",
        "        new_row[\"dayofmonth\"] = next_date.day\n",
        "        new_row[\"quarter\"] = (next_date.month - 1) // 3 + 1\n",
        "        new_row[\"is_weekend\"] = 1 if next_date.weekday() >= 5 else 0\n",
        "\n",
        "        for lag in [1, 7, 14]:\n",
        "            if len(hist_df) >= lag:\n",
        "                prev_val = hist_df[target_col].iloc[-lag]\n",
        "                pct_change = np.random.normal(0, variability_factor)\n",
        "                new_row[f\"{target_col}_lag_{lag}\"] = prev_val * (1 + pct_change)\n",
        "            else:\n",
        "                new_row[f\"{target_col}_lag_{lag}\"] = hist_df[target_col].mean()\n",
        "\n",
        "        for win in [7, 14]:\n",
        "            if len(hist_df) >= win:\n",
        "                prev_window = hist_df[target_col].iloc[-win:]\n",
        "                pct_changes = np.random.normal(0, variability_factor, size=win)\n",
        "                adjusted_window = prev_window * (1 + pct_changes)\n",
        "                new_row[f\"{target_col}_roll_mean_{win}\"] = adjusted_window.mean()\n",
        "                new_row[f\"{target_col}_roll_std_{win}\"] = adjusted_window.std(ddof=0)\n",
        "            else:\n",
        "                new_row[f\"{target_col}_roll_mean_{win}\"] = hist_df[target_col].mean()\n",
        "                new_row[f\"{target_col}_roll_std_{win}\"] = hist_df[target_col].std(ddof=0)\n",
        "\n",
        "        feature_cols = [col for col in hist_df.columns if col not in [\"date\", \"usage_cpu\", \"usage_storage\", \"users_active\", \"unique_id\"]]\n",
        "        X_next = new_row[feature_cols]\n",
        "\n",
        "        pred = model.predict(X_next)[0]\n",
        "\n",
        "        std_dev = new_row[[f\"{target_col}_roll_std_7\", f\"{target_col}_roll_std_14\"]].mean(axis=1).values[0]\n",
        "\n",
        "\n",
        "        lower = pred - z_score * std_dev\n",
        "        upper = pred + z_score * std_dev\n",
        "\n",
        "        if target_col == \"usage_cpu\":\n",
        "            upper = min(upper, 100)\n",
        "        lower = max(lower, 0)\n",
        "\n",
        "        forecasts.append({\n",
        "            \"date\": next_date.strftime(\"%Y-%m-%d\"),\n",
        "            \"predicted\": float(pred),\n",
        "            \"lower_95\": float(lower),\n",
        "            \"upper_95\": float(upper)\n",
        "        })\n",
        "\n",
        "        new_row[target_col] = pred\n",
        "        hist_df = pd.concat([hist_df, new_row], ignore_index=True)\n",
        "\n",
        "    return forecasts"
      ],
      "metadata": {
        "id": "QI8ZwETy-8A2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = forecast_next_30_days(model_cpu, \"usage_cpu\", encoded_insights, variability_factor=0.25)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1QqQxU3-xeP",
        "outputId": "b3fa0664-d304-4534-fbbe-49b5bceb04e2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'date': '2023-04-01',\n",
              "  'predicted': 73.275505612054,\n",
              "  'lower_95': 37.47420468615455,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-02',\n",
              "  'predicted': 78.06984721465562,\n",
              "  'lower_95': 40.35778392183062,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-03',\n",
              "  'predicted': 71.39372336767073,\n",
              "  'lower_95': 39.73636395732156,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-04',\n",
              "  'predicted': 79.93056298003795,\n",
              "  'lower_95': 47.6131479813415,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-05',\n",
              "  'predicted': 75.82876590187132,\n",
              "  'lower_95': 40.38249661730409,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-06',\n",
              "  'predicted': 63.96898872703402,\n",
              "  'lower_95': 21.69153840562565,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-07',\n",
              "  'predicted': 80.56755324530471,\n",
              "  'lower_95': 46.24219684360594,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-08',\n",
              "  'predicted': 74.50981959854893,\n",
              "  'lower_95': 32.86756925527948,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-09',\n",
              "  'predicted': 75.7527043855809,\n",
              "  'lower_95': 33.512926597847326,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-10',\n",
              "  'predicted': 71.98134014854284,\n",
              "  'lower_95': 23.524621581913806,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-11',\n",
              "  'predicted': 78.45295744667297,\n",
              "  'lower_95': 38.39208899452471,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-12',\n",
              "  'predicted': 76.57406814557197,\n",
              "  'lower_95': 37.844308422165675,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-13',\n",
              "  'predicted': 79.74288153339299,\n",
              "  'lower_95': 52.01637374022774,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-14',\n",
              "  'predicted': 69.10158904537056,\n",
              "  'lower_95': 44.97036180849345,\n",
              "  'upper_95': 93.23281628224767},\n",
              " {'date': '2023-04-15',\n",
              "  'predicted': 80.57676741797648,\n",
              "  'lower_95': 58.81086082973112,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-16',\n",
              "  'predicted': 74.61001019094698,\n",
              "  'lower_95': 47.71597167914193,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-17',\n",
              "  'predicted': 70.24620069123092,\n",
              "  'lower_95': 35.09339709998604,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-18',\n",
              "  'predicted': 73.37500831507137,\n",
              "  'lower_95': 30.06907705793185,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-19',\n",
              "  'predicted': 65.69780937549065,\n",
              "  'lower_95': 31.072843024534798,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-20',\n",
              "  'predicted': 80.10277334511744,\n",
              "  'lower_95': 42.62986270328864,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-21',\n",
              "  'predicted': 75.06362841328782,\n",
              "  'lower_95': 37.25302827392351,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-22',\n",
              "  'predicted': 80.2981571377823,\n",
              "  'lower_95': 54.166780747966286,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-23',\n",
              "  'predicted': 71.23779782790324,\n",
              "  'lower_95': 42.26983691925136,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-24',\n",
              "  'predicted': 72.78364875606538,\n",
              "  'lower_95': 35.15710271047393,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-25',\n",
              "  'predicted': 75.48365820467914,\n",
              "  'lower_95': 47.05231767127918,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-26',\n",
              "  'predicted': 66.63272729208568,\n",
              "  'lower_95': 26.395790685039508,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-27',\n",
              "  'predicted': 76.55088957666499,\n",
              "  'lower_95': 35.73192558821638,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-28',\n",
              "  'predicted': 70.03051200077016,\n",
              "  'lower_95': 30.982332268721507,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-29',\n",
              "  'predicted': 79.2328307164716,\n",
              "  'lower_95': 46.1032139283735,\n",
              "  'upper_95': 100.0},\n",
              " {'date': '2023-04-30',\n",
              "  'predicted': 71.21786802892159,\n",
              "  'lower_95': 36.391907224473876,\n",
              "  'upper_95': 100.0}]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = forecast_next_30_days(model_storage, \"usage_storage\", encoded_insights, variability_factor=0.25)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjsgbh5NEADk",
        "outputId": "526c7ea2-e8b4-47e4-fa9c-53a1f0ed24d4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'date': '2023-04-01',\n",
              "  'predicted': 1273.6056395238088,\n",
              "  'lower_95': 329.840698985024,\n",
              "  'upper_95': 2217.3705800625935},\n",
              " {'date': '2023-04-02',\n",
              "  'predicted': 1273.594704158244,\n",
              "  'lower_95': 641.0903182588185,\n",
              "  'upper_95': 1906.0990900576694},\n",
              " {'date': '2023-04-03',\n",
              "  'predicted': 1272.132539671274,\n",
              "  'lower_95': 441.45446063951545,\n",
              "  'upper_95': 2102.8106187030326},\n",
              " {'date': '2023-04-04',\n",
              "  'predicted': 1274.2975471575312,\n",
              "  'lower_95': 441.04059131627116,\n",
              "  'upper_95': 2107.554502998791},\n",
              " {'date': '2023-04-05',\n",
              "  'predicted': 1274.0630567506264,\n",
              "  'lower_95': 378.77365261334023,\n",
              "  'upper_95': 2169.3524608879125},\n",
              " {'date': '2023-04-06',\n",
              "  'predicted': 1274.4072825422975,\n",
              "  'lower_95': 472.0406329585013,\n",
              "  'upper_95': 2076.7739321260938},\n",
              " {'date': '2023-04-07',\n",
              "  'predicted': 1274.0247148475491,\n",
              "  'lower_95': 702.4408416804628,\n",
              "  'upper_95': 1845.6085880146356},\n",
              " {'date': '2023-04-08',\n",
              "  'predicted': 1272.6245744331325,\n",
              "  'lower_95': 764.7044878929365,\n",
              "  'upper_95': 1780.5446609733285},\n",
              " {'date': '2023-04-09',\n",
              "  'predicted': 1274.0537559741183,\n",
              "  'lower_95': 690.5676641380002,\n",
              "  'upper_95': 1857.5398478102366},\n",
              " {'date': '2023-04-10',\n",
              "  'predicted': 1273.0837667505386,\n",
              "  'lower_95': 514.7442053227351,\n",
              "  'upper_95': 2031.4233281783422},\n",
              " {'date': '2023-04-11',\n",
              "  'predicted': 1271.869711408503,\n",
              "  'lower_95': 519.1644933263455,\n",
              "  'upper_95': 2024.5749294906605},\n",
              " {'date': '2023-04-12',\n",
              "  'predicted': 1271.3702284204774,\n",
              "  'lower_95': 631.9106301876444,\n",
              "  'upper_95': 1910.8298266533102},\n",
              " {'date': '2023-04-13',\n",
              "  'predicted': 1272.763632500841,\n",
              "  'lower_95': 881.94414488915,\n",
              "  'upper_95': 1663.5831201125318},\n",
              " {'date': '2023-04-14',\n",
              "  'predicted': 1272.5057691957688,\n",
              "  'lower_95': 872.9132445469293,\n",
              "  'upper_95': 1672.0982938446084},\n",
              " {'date': '2023-04-15',\n",
              "  'predicted': 1273.0088590712603,\n",
              "  'lower_95': 892.5365178864113,\n",
              "  'upper_95': 1653.4812002561093},\n",
              " {'date': '2023-04-16',\n",
              "  'predicted': 1273.7246797740597,\n",
              "  'lower_95': 757.6271831704704,\n",
              "  'upper_95': 1789.8221763776492},\n",
              " {'date': '2023-04-17',\n",
              "  'predicted': 1272.970312283834,\n",
              "  'lower_95': 604.8729260059949,\n",
              "  'upper_95': 1941.0676985616728},\n",
              " {'date': '2023-04-18',\n",
              "  'predicted': 1274.432528114434,\n",
              "  'lower_95': 539.8626751043593,\n",
              "  'upper_95': 2009.0023811245087},\n",
              " {'date': '2023-04-19',\n",
              "  'predicted': 1271.2806472521197,\n",
              "  'lower_95': 693.5060527056174,\n",
              "  'upper_95': 1849.0552417986219},\n",
              " {'date': '2023-04-20',\n",
              "  'predicted': 1274.123743067856,\n",
              "  'lower_95': 580.4542501332296,\n",
              "  'upper_95': 1967.7932360024822},\n",
              " {'date': '2023-04-21',\n",
              "  'predicted': 1274.3453928247907,\n",
              "  'lower_95': 615.0930901593541,\n",
              "  'upper_95': 1933.5976954902271},\n",
              " {'date': '2023-04-22',\n",
              "  'predicted': 1272.9089478128747,\n",
              "  'lower_95': 849.6693581763432,\n",
              "  'upper_95': 1696.1485374494061},\n",
              " {'date': '2023-04-23',\n",
              "  'predicted': 1272.8901750037962,\n",
              "  'lower_95': 792.0344058427352,\n",
              "  'upper_95': 1753.7459441648573},\n",
              " {'date': '2023-04-24',\n",
              "  'predicted': 1273.8236342739206,\n",
              "  'lower_95': 643.5075014053419,\n",
              "  'upper_95': 1904.1397671424993},\n",
              " {'date': '2023-04-25',\n",
              "  'predicted': 1272.6721451066483,\n",
              "  'lower_95': 829.0898435812524,\n",
              "  'upper_95': 1716.2544466320442},\n",
              " {'date': '2023-04-26',\n",
              "  'predicted': 1273.3273004298267,\n",
              "  'lower_95': 568.2337523864032,\n",
              "  'upper_95': 1978.42084847325},\n",
              " {'date': '2023-04-27',\n",
              "  'predicted': 1272.5832597970539,\n",
              "  'lower_95': 637.6457816053781,\n",
              "  'upper_95': 1907.5207379887297},\n",
              " {'date': '2023-04-28',\n",
              "  'predicted': 1272.0758532238867,\n",
              "  'lower_95': 585.572620593483,\n",
              "  'upper_95': 1958.5790858542905},\n",
              " {'date': '2023-04-29',\n",
              "  'predicted': 1273.0290470211182,\n",
              "  'lower_95': 704.193579906735,\n",
              "  'upper_95': 1841.8645141355014},\n",
              " {'date': '2023-04-30',\n",
              "  'predicted': 1272.8395957928685,\n",
              "  'lower_95': 684.5713485736024,\n",
              "  'upper_95': 1861.1078430121347}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = forecast_next_30_days(model_users, \"users_active\", encoded_insights, variability_factor=0.25)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeVwS977_WWB",
        "outputId": "f921d437-f0e9-41c8-9f17-8b54a7c173cc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'date': '2023-04-01',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 113.34440270982671,\n",
              "  'upper_95': 589.8628726807983},\n",
              " {'date': '2023-04-02',\n",
              "  'predicted': 350.11480712890625,\n",
              "  'lower_95': 102.64370697571917,\n",
              "  'upper_95': 597.5859072820933},\n",
              " {'date': '2023-04-03',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 174.10950907865924,\n",
              "  'upper_95': 529.0977663119658},\n",
              " {'date': '2023-04-04',\n",
              "  'predicted': 350.11480712890625,\n",
              "  'lower_95': 124.37542951055889,\n",
              "  'upper_95': 575.8541847472536},\n",
              " {'date': '2023-04-05',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 200.8697316905907,\n",
              "  'upper_95': 502.3375437000343},\n",
              " {'date': '2023-04-06',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 120.19746454279587,\n",
              "  'upper_95': 583.0098108478292},\n",
              " {'date': '2023-04-07',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 157.14934876936286,\n",
              "  'upper_95': 546.0579266212621},\n",
              " {'date': '2023-04-08',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 147.71390433461116,\n",
              "  'upper_95': 555.4933710560139},\n",
              " {'date': '2023-04-09',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 177.4240986833145,\n",
              "  'upper_95': 525.7831767073105},\n",
              " {'date': '2023-04-10',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 137.8767899345042,\n",
              "  'upper_95': 565.3304854561209},\n",
              " {'date': '2023-04-11',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 163.43885154380988,\n",
              "  'upper_95': 539.7684238468152},\n",
              " {'date': '2023-04-12',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 163.267361404554,\n",
              "  'upper_95': 539.939913986071},\n",
              " {'date': '2023-04-13',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 241.923280849658,\n",
              "  'upper_95': 461.283994540967},\n",
              " {'date': '2023-04-14',\n",
              "  'predicted': 354.2467956542969,\n",
              "  'lower_95': 239.21138392109617,\n",
              "  'upper_95': 469.2822073874976},\n",
              " {'date': '2023-04-15',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 246.13547268156992,\n",
              "  'upper_95': 457.0718027090551},\n",
              " {'date': '2023-04-16',\n",
              "  'predicted': 350.11480712890625,\n",
              "  'lower_95': 208.00423904893282,\n",
              "  'upper_95': 492.22537520887965},\n",
              " {'date': '2023-04-17',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 167.20371014589028,\n",
              "  'upper_95': 536.0035652447348},\n",
              " {'date': '2023-04-18',\n",
              "  'predicted': 354.2467956542969,\n",
              "  'lower_95': 151.72660519278787,\n",
              "  'upper_95': 556.7669861158058},\n",
              " {'date': '2023-04-19',\n",
              "  'predicted': 354.2467956542969,\n",
              "  'lower_95': 194.70834192605,\n",
              "  'upper_95': 513.7852493825437},\n",
              " {'date': '2023-04-20',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 157.7654730578043,\n",
              "  'upper_95': 545.4418023328208},\n",
              " {'date': '2023-04-21',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 168.02070726904242,\n",
              "  'upper_95': 535.1865681215826},\n",
              " {'date': '2023-04-22',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 234.4934365845266,\n",
              "  'upper_95': 468.7138388060984},\n",
              " {'date': '2023-04-23',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 218.20270911802874,\n",
              "  'upper_95': 485.00456627259626},\n",
              " {'date': '2023-04-24',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 177.40527795922512,\n",
              "  'upper_95': 525.8019974313999},\n",
              " {'date': '2023-04-25',\n",
              "  'predicted': 354.2467956542969,\n",
              "  'lower_95': 231.05375239250034,\n",
              "  'upper_95': 477.43983891609344},\n",
              " {'date': '2023-04-26',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 156.1476489478756,\n",
              "  'upper_95': 547.0596264427494},\n",
              " {'date': '2023-04-27',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 176.7780069361488,\n",
              "  'upper_95': 526.4292684544762},\n",
              " {'date': '2023-04-28',\n",
              "  'predicted': 354.2467956542969,\n",
              "  'lower_95': 164.50127029397524,\n",
              "  'upper_95': 543.9923210146185},\n",
              " {'date': '2023-04-29',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 194.9660967108203,\n",
              "  'upper_95': 508.2411786798047},\n",
              " {'date': '2023-04-30',\n",
              "  'predicted': 351.6036376953125,\n",
              "  'lower_95': 188.02474669015643,\n",
              "  'upper_95': 515.1825287004685}]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}